<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Machine Learning Project</title>

    <!-- Bootstrap Core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link href="css/shop-item.css" rel="stylesheet">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>

<body>

    <!-- Navigation -->
    <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
        <div class="container">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand" href="#">Yelp User Usefulness Prediction</a>
            </div>
            <!-- Collect the nav links, forms, and other content for toggling 
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav">
                    <li>
                        <a href="#">About</a>
                    </li>
                    <li>
                        <a href="#">Services</a>
                    </li>
                    <li>
                        <a href="#">Contact</a>
                    </li>
                </ul>
            </div>-->
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container -->
    </nav>

    <!-- Page Content -->
    <div class="container">

        <div class="row">

            <div class="col-md-3">
                <p class="lead">Directory</p>
                <div class="list-group">
                    <a href="index.html" class="list-group-item">Home</a>
                    <a href="fullreport.html" class="list-group-item active">Full Report</a>
                    <a href="contact.html" class="list-group-item">Contact</a>
                </div>
            </div>

            <div class="col-md-9">

                <div class="thumbnail">
                    <img class="img-responsive" alt="">
                    <div class="caption-full">
                        <h4><a href="#">Full Report Abstract</a></h4>
                        <h5><strong>Problem Outline</strong></h5>
                        <p>Our task is to create a predictor of the helpfulness of Yelp users based on a user’s past Yelp reviews and account history.  This task is important because many reviewers on yelp are false accounts made to boost a business’ review score - this would help to filter out such accounts. In addition, Yelp could use the information to display the posts from the most helpful users near the top of reviews for a business or to create a “trusted reviews” section.</p>

                        <h4><strong>Tools</strong></h4>
                        <p>Current software packages that the team is using include the following:</p>
                        <ul>
                            <li>Anaconda Package - A collection of various science packages for python.  From the Anaconda Package we have used numpy, sklearn, pandas, textblob, NLTK.</li>
                            <li>Python 2.7 / Python 3.4</li>
                            <li>Weka</li>
                        </ul>

                        <h4><strong>Dataset: Attributes/Features</strong></h4>
                        <p>We have divided our problem into 2 main approaches: inclusion of community provided attributes (votes from other users) and exclusion of such attributes. Exclusion allows investigating into a portion of the data untouched by the community - we deemed a concurrent investigation and comparison of the 2 would be interesting to pursue. </p>
                        <ul>The full list of attributes used in the exclusion set are:
                            <li>Review count of user</li>
                            <li>Yelping for period (how long has it been since user registration)</li>
                            <li>Stars given by user (average and std)</li>
                            <li>Polarity of texts of user (average and std)</li>
                            <li>Subjectivity of texts of user (average and std)</li>
                            <li>Review text word count (average and std)</li>
                            <li>The star rating of the business the user reviewed (average and std)</li>
                            <li>The number of reviews of the business the user reviewed (average and std)</li>
                            <li>Swear count (average)</li>
                            <li>Vocabulary density (average: ratio of unique words to the total number of words in a review)</li>
                            <li>Punctuation density (average: ratio of punctuation marks to total words in a review)</li>
                        </ul>
                        <ul>The inclusive set adds 3 additional attributes to the ones above:
                            <li>Number of “funny” votes for reviews (average)</li>
                            <li>Number of “cool” votes for reviews (average)</li>
                            <li>Number of fans</li>
                        </ul>
                        <p>The output is user "usefulness" defined as a boolean true or false for being a useful user or not.</p>
                        <p>Defining usefulness of a user was the key aim to our task. We settled upon a normalised ratio between the number of ‘useful’ votes a user gained from their reviews against their total number of reviews. We then decided on a threshold to create a boolean ‘user_helpful’ value that we could use for binary classification. </p>
                        <p>However, we were given the impression that the threshold may skew our results in a certain direction. Hence we went the direction of producing multiple results of the same process, just using different thresholds to divide the data. </p>
                        
                        <h5><strong>Training / Test Method</strong></h5>
                        <p>The team obtained data sets from the annual Yelp dataset challenge. The original user dataset consists of roughly 39,000 data points, the review dataset of over 1,500,000, and the business set of roughly 60,000. The user dataset was joined with the reviews and enterprise data sets to form a larger user dataset of one file. Natural Language Processing and insights made into attribute relationships throughout our iterations were used to produce additional custom attributes in an attempt to extract more information from the data collected.
</p>
                        <p>However, we realised during our iterations that having a significantly low partition of ‘true’ or ‘false’ data points could skew our models. Hence, we went through further processing to make sure that we had the same number of ‘true’ and ‘false’ data points for each threshold test.
</p>
                        <p>The overall training and validation set was then made by splitting the resulting threshold set into two equal parts. We felt having a roughly equal data set would give us statistically more stable results. Our resulting dataset size for threshold 1 was 19,000 (for both training and validation) and roughly 7000 for threshold 2.
</p>
                        
                        <h5><strong>Training/Validation Results</strong></h5>
                        <h6><strong>Algorithm Choices</strong></h6>
                        <p>ZeroR was used as a baseline for comparison. J48 was selected to see if there existed attributes of dominant information gain. Bayes Net was chosen to see if there existed causality relationships between attributes, with Naive Bayes used in conjunction for comparison. IBk was selected to see if there exists any general correlations in data. 
</p>
                        
                        <h5><strong>Graphs</strong></h5>
                        
                        <div>
                            <img class="col-md-5" text-align="center" align="center" style="width: 50%; float: left;" src="train_thresh1.png"/>
                            <img class="col-md-5" text-align="center" align="center" style="width: 50%; float: left;" src="train_thresh2.png"/> 
                            <img text-align="center" align="center" style="width: 50%; float: left;" src="valid_set_thresh1.png"/>
                            <img text-align="center" align="center" style="width: 50%; float: left;" src="valid_set_thresh2.png"/>
                        </div>
                        
                        
                        <h6 style="position:relative; top:10px;"><strong>Analysis</strong></h6>
                        <p>As expected, the results from the “inclusive” set had higher accuracy (approx. 95% for each threshold) - the funny vote and cool votes indeed seemed tightly coupled with our target attribute. Decision trees (75%) and Bayes Net (68%) performed the best out of all the applied machine learning algorithms overall. J48 has a slight loss of about 4~8% on the validation set depending on the threshold, but seems to show that it was less of an overfit, which would be important for larger data sets.</p>
                        <p>Taking a closer look at the nodes the j48 trees would split on, interestingly, it seems that the number of stars of the business reviewed, as well as the number of the reviews the business had, was a factor on deciding user usefulness. The data seems to express that under similar user profile conditions, users were more likely to say that a review was helpful if the business being reviewed had a higher number of stars or if the business already had a large number of reviews.</p>
                        <p>In general the accuracies for threshold 2 were about 10% greater than in threshold 1 in the training sets. In the validation sets, the accuracies were about the same - thus suggesting the existence of a correlation regardless of threshold bounds. In terms of the validation set, j48 seemed to outdo BayesNet for the inclusion set (95% vs. 68%) while both the models rested closely against the 70% line for the exclusion set. The findings conclude a useful or significant relationship between the user provided votes and our desired output function, whereas the exclusion of user provided votes results in no significant attributes nor relationship.  However, a correlation may be easily determined by a combination of attributes (ex. review count and reviewed business information) and latching upon their relative correlations.</p>
                        
                        
                        <h5><strong>Suggestions for Future Work</strong></h5>
                        <p>From observation, Yelp seems to employ a custom algorithm to differentiate between good recommendations and bad. A comparison into this algorithm may prove to be of use and may inspire the generation of further possible attributes. </p>
                        <p>Natural Language Processing did not seem to yield sufficient results for our project - if there is more time, it might be interesting to scrape the websites of businesses and compare that with text information in the reviews. This may provide a useful dataset to construct an insight into ‘market speech’ - a classification we might be able to apply on the text data to filter out more accounts. </p>
                        <p>Finally, judging whether a user is helpful and judging whether a review is helpful are 2 facets of the ‘usefulness’ question that our group did not pursue to compare - it would be interesting if further work could be done in this direction. </p>
                    </div>
                </div>

              

            </div>

        </div>

    </div>
    <!-- /.container -->

    <div class="container">

        <hr>

        <!-- Footer -->
        <!--
        <footer>
            <div class="row">
                <div class="col-lg-12">
                    <p>Copyright &copy; Your Website 2014</p>
                </div>
            </div>
        </footer>
        -->

    </div>
    <!-- /.container -->

    <!-- jQuery -->
    <script src="js/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="js/bootstrap.min.js"></script>

</body>

</html>
